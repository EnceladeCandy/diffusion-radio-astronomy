{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch.func import vmap, grad\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import MultivariateNormal\n",
    "import h5py\n",
    "from score_models import ScoreModel, NCSNpp\n",
    "import json\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "sys.path.append(\"..\\\\\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#scratch/diffusion-radio-astronomy/notebooks/real_posterior.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 414/500 [00:09<00:02, 38.59it/s]"
     ]
    }
   ],
   "source": [
    "# Importing the models hparams and weights\n",
    "file = open(\"/home/noedia/projects/rrg-lplevass/data/score_models/ncsnpp_probes_g_64_230604024652/model_hparams.json\")\n",
    "model_hparams = json.load(file)\n",
    "sigma_min, sigma_max = model_hparams[\"sigma_min\"], model_hparams[\"sigma_max\"]\n",
    "\n",
    "torch.manual_seed(2)\n",
    "score_model = ScoreModel(checkpoints_directory=\"/home/noedia/projects/rrg-lplevass/data/score_models/ncsnpp_probes_g_64_230604024652\")\n",
    "x = score_model.sample(1, shape = (1, 64, 64), steps = 500)[0,0]\n",
    "img_size = img.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x.cpu(), cmap = \"hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a psf for 64 * 64 images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(x, target_size=64):\n",
    "    x_size = x.shape[-1] \n",
    "    start = int((x_size-target_size)/2)\n",
    "    end = start + target_size\n",
    "    x = x[start:end, start:end]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = img.shape[-1]\n",
    "# Path to your .fits file\n",
    "fits_file_path = '../../psf_256.fits'\n",
    "\n",
    "# Open the .fits file using Astropy\n",
    "with fits.open(fits_file_path) as hdul:\n",
    "    # Get the header and data from the primary HDU (Extension 0)\n",
    "    header = hdul[0].header\n",
    "    psf = torch.tensor((hdul[0].data).astype(np.float32))[0,0, ...].to(device)\n",
    "\n",
    "psf = resize(psf, target_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4096) must match the size of tensor b (65536) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m vis_full \u001b[38;5;241m=\u001b[39m ft(x)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      8\u001b[0m sampling_function\u001b[38;5;241m=\u001b[39m ft(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mifftshift(psf))\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m----> 9\u001b[0m vis_sampled \u001b[38;5;241m=\u001b[39m \u001b[43msampling_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvis_full\u001b[49m\n\u001b[1;32m     11\u001b[0m sigma_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-1\u001b[39m\n\u001b[1;32m     12\u001b[0m vis_sampled \u001b[38;5;241m=\u001b[39m vis_sampled\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4096) must match the size of tensor b (65536) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "def ft(x): \n",
    "    return torch.fft.fft2(x, norm = \"ortho\")\n",
    "\n",
    "def ift(x): \n",
    "    return torch.fft.ifft2(x, norm = \"ortho\")\n",
    "\n",
    "vis_full = ft(x).flatten()\n",
    "sampling_function= ft(torch.fft.ifftshift(psf)).flatten()\n",
    "vis_sampled = sampling_function * vis_full\n",
    "\n",
    "sigma_likelihood = 1e-1\n",
    "vis_sampled = vis_sampled.flatten()\n",
    "vis_sampled = torch.cat([vis_sampled.real, vis_sampled.imag])\n",
    "\n",
    "y_dim = len(vis_sampled)  \n",
    "dist_likelihood = MultivariateNormal(loc = torch.zeros(y_dim).to(device), covariance_matrix=sigma_likelihood **2 * torch.eye(y_dim).to(device))\n",
    "eta = dist_likelihood.sample([])\n",
    "\n",
    "y = vis_sampled + eta \n",
    "\n",
    "\n",
    "\n",
    "# PLOTS##################\n",
    "fig, axs = plt.subplots(1, 2, figsize = (10, 4))\n",
    "dirty_image_noise = ift((y[:img_size**2] + 1j * y[img_size**2:]).reshape(img_size, img_size)).real\n",
    "\n",
    "\n",
    "axs[0].imshow(x.reshape(img_size, img_size).cpu(), cmap = \"hot\")\n",
    "axs[0].set_title(\"Ground-truth\")\n",
    "axs[1].imshow(dirty_image_noise.cpu(), cmap = \"hot\")\n",
    "axs[1].set_title(\"Dirty image with noise\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 4.00 GiB total capacity; 952.89 MiB already allocated; 1.14 GiB free; 996.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn([\u001b[39m10\u001b[39m, img_size \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m])\u001b[39m.\u001b[39mto(device) \n\u001b[0;32m     28\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(size \u001b[39m=\u001b[39m [\u001b[39m10\u001b[39m, \u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 30\u001b[0m \u001b[39mprint\u001b[39m((torch\u001b[39m.\u001b[39msum(score_likelihood(x, t)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(score_likelihood(x, t))\n",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m, in \u001b[0;36mscore_likelihood\u001b[1;34m(x, t)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore_likelihood\u001b[39m(x, t): \n\u001b[1;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m vmap(grad(\u001b[39mlambda\u001b[39;49;00m x, t: logprob_likelihood(y\u001b[39m-\u001b[39;49m f(x),  (sigma_likelihood \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m+\u001b[39;49m sigma(t)\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m) \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49meye(y_dim, device \u001b[39m=\u001b[39;49m x\u001b[39m.\u001b[39;49mdevice))))(x, t)\n",
      "File \u001b[1;32mc:\\Users\\noedi\\anaconda3\\lib\\site-packages\\torch\\_functorch\\vmap.py:434\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[0;32m    431\u001b[0m                          args_spec, out_dims, randomness, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    433\u001b[0m \u001b[39m# If chunk_size is not specified.\u001b[39;00m\n\u001b[1;32m--> 434\u001b[0m \u001b[39mreturn\u001b[39;00m _flat_vmap(\n\u001b[0;32m    435\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    436\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\noedi\\anaconda3\\lib\\site-packages\\torch\\_functorch\\vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     38\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\noedi\\anaconda3\\lib\\site-packages\\torch\\_functorch\\vmap.py:619\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[1;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[1;32m--> 619\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39mbatched_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    620\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[0;32m    621\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\noedi\\anaconda3\\lib\\site-packages\\torch\\_functorch\\eager_transforms.py:1380\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m   1379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 1380\u001b[0m     results \u001b[39m=\u001b[39m grad_and_value(func, argnums, has_aux\u001b[39m=\u001b[39mhas_aux)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1381\u001b[0m     \u001b[39mif\u001b[39;00m has_aux:\n\u001b[0;32m   1382\u001b[0m         grad, (_, aux) \u001b[39m=\u001b[39m results\n",
      "File \u001b[1;32mc:\\Users\\noedi\\anaconda3\\lib\\site-packages\\torch\\_functorch\\vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     38\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\noedi\\anaconda3\\lib\\site-packages\\torch\\_functorch\\eager_transforms.py:1245\u001b[0m, in \u001b[0;36mgrad_and_value.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1242\u001b[0m diff_args \u001b[39m=\u001b[39m _slice_argnums(args, argnums, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1243\u001b[0m tree_map_(partial(_create_differentiable, level\u001b[39m=\u001b[39mlevel), diff_args)\n\u001b[1;32m-> 1245\u001b[0m output \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1246\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n\u001b[0;32m   1247\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(output, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(output) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m, in \u001b[0;36mscore_likelihood.<locals>.<lambda>\u001b[1;34m(x, t)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore_likelihood\u001b[39m(x, t): \n\u001b[1;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m vmap(grad(\u001b[39mlambda\u001b[39;00m x, t: logprob_likelihood(y\u001b[39m-\u001b[39m f(x),  (sigma_likelihood \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m+\u001b[39;49m sigma(t)\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m) \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49meye(y_dim, device \u001b[39m=\u001b[39;49m x\u001b[39m.\u001b[39;49mdevice))))(x, t)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 4.00 GiB total capacity; 952.89 MiB already allocated; 1.14 GiB free; 996.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "def beta(t): \n",
    "    return 0.1 + (20-0.1) * t\n",
    "def int_beta(t): \n",
    "    return 0.1 * t + (20 - 0.1) * t **2 / 2\n",
    " \n",
    "def sigma(t): \n",
    "    return torch.sqrt(1-torch.exp(-int_beta(t)))\n",
    "\n",
    "def logprob_likelihood(x, cov_mat): \n",
    "    dist = torch.distributions.MultivariateNormal(loc = torch.zeros(y_dim, device = y.device), covariance_matrix = cov_mat, validate_args=False)\n",
    "    return dist.log_prob(x)\n",
    "\n",
    "def f(x): \n",
    "    vis_full = ft(x.reshape(img_size, img_size)).flatten()\n",
    "    vis_sampled = sampling_function * vis_full\n",
    "\n",
    "    vis_sampled = torch.cat([vis_sampled.real, vis_sampled.imag])\n",
    "    return vis_sampled\n",
    "\n",
    "def score_likelihood(x, t): \n",
    "    return vmap(grad(lambda x, t: logprob_likelihood(y- f(x),  (sigma_likelihood ** 2 + sigma(t)**2) * torch.eye(y_dim, device = x.device))))(x, t)\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "def score_posterior(x, t): \n",
    "    return score_model.score(t, x.reshape(-1, 1, img_size, img_size)).flatten(start_dim = 1) + score_likelihood(x, t)\n",
    "\n",
    "x = torch.randn([10, img_size ** 2]).to(device) \n",
    "t = torch.ones(size = [10, 1]).to(device)\n",
    "\n",
    "print((torch.sum(score_likelihood(x, t)**2))**0.5)\n",
    "print(score_likelihood(x, t))\n",
    "#logprob_likelihood(y-f2(x), sigma_likelihood**2 * torch.eye(y_dim, device = x.device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffusion",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
